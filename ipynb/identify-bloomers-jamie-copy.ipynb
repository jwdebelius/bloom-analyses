{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the candidate blooming bacteria\n",
    "\n",
    "The goal of this notebook is to identify candidate bacteria (as observed by 16S rRNA V4 reads) which appear to bloom under shipping conditions. We identify candidates in two parts: first, we'll look for bacteria which grew at room temperature in storage studies; second, we'll look at shipped American Gut fecal samples and compared them to fresh-frozen fecal samples. We use the combined technique because the storage studies were limited in sample size and individuals, while the comparison with fresh-frozen studies offers many more individuals and samples. \n",
    "\n",
    "Briefly, this process will involve the following steps:\n",
    "\n",
    "1. Load the datasets:\n",
    "\n",
    "    1. Preservation study (Song [1](#1); Qiita ID [10389](https://qiita.ucsd.edu/study/description/10394))\n",
    "    2. Mayo Clinic Fecal Stability (Mayo [2](#2); Qiita ID [10184](https://qiita.ucsd.edu/study/description/10184))\n",
    "    3. The American Gut Project (AGP; Qiita ID [10317](https://qiita.ucsd.edu/study/description/10317))\n",
    "    4. The Personal Genome Project (PGP; Qiita ID [1189](https://qiita.ucsd.edu/study/description/1189))\n",
    "    5. Human Genetics and the Gut Microbiome (Twins [3](#3); Qiita ID [2014](https://qiita.ucsd.edu/study/description/2014))\n",
    "    6. A Whole Grain diet study (ERC [4](#4); Qiita ID [1481](https://qiita.ucsd.edu/study/description/1481))\n",
    "    \n",
    "2. Calculate the fold change per sOTU in the storage studies (Mayo and Song) over time\n",
    "3. Calculate the fold change in the AGP compared to fresh-frozen studies (PGP, Twins, and ERC)\n",
    "4. Identify candidate blooming bacteria by combining the fold changes in steps 2 and 3\n",
    "5. Plot the scatter of the fold change per sOTU with highlighting the candidate blooming bacteria\n",
    "6. Plot taxonomy plots before and after filtering for blooms on the American Gut and fresh frozen studies.\n",
    "\n",
    "Let's start by loading the necessary Python functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "import matplotlib as mpl\n",
    "# load modules used in the analysis\n",
    "import biom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import skbio\n",
    "from skbio.stats.composition import closure\n",
    "\n",
    "\n",
    "from bloom.stats import permutation_mean\n",
    "\n",
    "# plots inside the notebook\n",
    "%matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "# set the random seed\n",
    "np.random.seed(2016)\n",
    "\n",
    "# set visualization column size for pandas\n",
    "pd.set_option('max_colwidth', 20)\n",
    "sn.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def exploding_panda(_bt):\n",
    "    \"\"\"BIOM to pandas DataFrame \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _bt : biom.Table\n",
    "        BIOM table\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The BIOM table converted into a DataFrame\n",
    "        object.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Based on this answer on SO:\n",
    "    http://stackoverflow.com/a/17819427/379593\n",
    "    \"\"\"\n",
    "    m = _bt.matrix_data\n",
    "    data = m.toarray()\n",
    "    return pd.DataFrame(data, index=_bt.ids('observation'),\n",
    "                        columns=_bt.ids('sample'))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading all experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a quick function to load the data. We'll load the BIOM table (`biom_fp`). We'll load the specified metadata columns (`meta_fp` and `keep_columns`). Then, we'll filter the data to include only samples with at least 1000 counts/sample (`depth`) and that fit the filtering criteria (`filters`). Finally, we'll add information about the study and preservation method to make later analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_studies(biom_fp, meta_fp, name, preservation, keep_columns=None, \n",
    "                 filters=None, depth=1000):\n",
    "    \"\"\"Loads the studies more easily\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    biom_fp : str\n",
    "        The filepath to the BIOM table\n",
    "    meta_fp : str\n",
    "        The filepath to the mapping file\n",
    "    name : str\n",
    "        The name of the study\n",
    "    preservation : {\"NONE\", \"STORAGE\", \"FRESH_FROZEN\"}\n",
    "        Denotes what preservation method, if any, was used.\n",
    "    keep_columns : list of str\n",
    "        The columns within the mapping file to retain.\n",
    "    filters : dict\n",
    "        Any filters to apply to the metadata\n",
    "    depth : int\n",
    "        A sequencing depth to rarefy to.\n",
    "    \"\"\"\n",
    "    if filters is None:\n",
    "        filters = {}\n",
    "        \n",
    "    # Loads the mapping file\n",
    "    if keep_columns == None:\n",
    "        meta = pd.read_csv(meta_fp, sep='\\t', dtype=str)\n",
    "    else:\n",
    "        meta = pd.read_csv(meta_fp, sep='\\t', dtype=str, usecols=keep_columns)\n",
    "    meta.set_index('#SampleID', inplace=True)\n",
    "    \n",
    "    # Loads the biom table\n",
    "    sotu = biom.load_table(biom_fp)\n",
    "\n",
    "    # We'll first filter the table by sequencing depth\n",
    "    sotu = sotu.filter(lambda val, id_, md: val.sum() > depth, axis='sample')\n",
    "    meta = meta.loc[sotu.ids(axis='sample')]\n",
    "    \n",
    "    # Filters the samples\n",
    "    def filter_meta(x):\n",
    "        return np.all([x[k] == v for k, v in filters.items()])\n",
    "    \n",
    "    meta_f = meta.loc[meta.apply(filter_meta, axis=1)].copy()\n",
    "    meta_f['STUDY'] = name\n",
    "    meta_f['PRESERVATION'] = preservation\n",
    "    sotu_f = sotu.filter(meta_f.index)\n",
    "    \n",
    "    return meta_f, sotu_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song fecal stability study\n",
    "\n",
    "The Song fecal stability study [[1](#1)] compares the stability of human and dog fecal samples under a variety of storage conditions. For the purposes of candidate bloom identification *for human specimens*, we will only use samples which originated from humans.\n",
    "\n",
    "We will only use samples with no preservative stored at ambient temperature for 7, 14 and 28 days (compared to samples sequenced at day 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_meta, song_sotu = load_studies(\n",
    "    biom_fp = '../data/storage.sejin.clean.withtax.biom',\n",
    "    meta_fp = '../data/map.storage.sejin.txt',\n",
    "    name='SONG',\n",
    "    preservation='STORAGE',\n",
    "    keep_columns=['#SampleID', 'preservative', 'temperature', 'temp2', 'species', \n",
    "                  'time_numeric', 'subject'],\n",
    "    filters={'preservative': 'None',\n",
    "             'temperature': 'amb',\n",
    "             'species': 'human'},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample is filtered to only include perservative-free samples from humans stored at room temperature. These include samples collected at day 0 and immediately sequenced, which we use as a reference. We will only use samples with at least 1000 sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mayo fecal stability study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mayo fecal stability study [[2](#2)] compares the effect of different storage conditions on human fecal samples. Samples were stored at room temperature with or without perservatives. We'll only look at samples without perservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mayo_meta, mayo_sotu = load_studies(\n",
    "    biom_fp = '../data/mayo.new.min10.clean.withtax.biom',\n",
    "    meta_fp = '../data/map.mayo1.txt',\n",
    "    name='MAYO_STORAGE',\n",
    "    preservation='STORAGE',\n",
    "    filters={'TREATMENT': 'No Additive'},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Gut\n",
    "\n",
    "The [American Gut Project](<http://www.americangut.org>) is a cross sectional study. Samples were shipped through local post without perservatives. We will use only human fecal samples with at least 1000 sequences per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agp_meta, agp_sotu = load_studies(biom_fp='../data/ag.qiita-10317.biom',\n",
    "                                  meta_fp='../data/ag.qiita-10317.txt',\n",
    "                                  name='AGP',\n",
    "                                  preservation='NONE',\n",
    "                                  keep_columns=['BODY_HABITAT', '#SampleID', 'HOST_COMMON_NAME'],\n",
    "                                  filters={'BODY_HABITAT': 'UBERON:feces', \n",
    "                                           'HOST_COMMON_NAME':'human'}\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UK twins study\n",
    "\n",
    "We'll use data from Goodrich et al [[3](#3)], a fresh frozen study comparing the heritability of the microbiome in monozygotic and dizygotic twins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twin_meta, twin_sotu = load_studies(biom_fp='../data/twins.qiita-2014.biom',\n",
    "                                meta_fp='../data/twins.qiita-2014.txt',\n",
    "                                name='UK_TWINS',\n",
    "                                preservation='FRESH_FROZEN',\n",
    "                                keep_columns=['env_matter', '#SampleID'],\n",
    "                                filters={'env_matter': 'ENVO:feces'}\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGP\n",
    "\n",
    "(unpublished), Qiita study ID [1189](<https://qiita.ucsd.edu/study/description/1189>)\n",
    "\n",
    "Samples were collected from participants in the Personal Genome Project and fresh frozen. The study has not yet been published, but data is avaliable in [Qiita study 1189](<https://qiita.ucsd.edu/study/description/1189>). The study contains fecal, oral and skin samples. We'll use only the fecal samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pgp_meta, pgp_sotu = load_studies(biom_fp='../data/pgp.qiita-1189.biom',\n",
    "                                  meta_fp='../data/pgp.qiita-1189.txt',\n",
    "                                  name='PGP',\n",
    "                                  preservation='FRESH_FROZEN',\n",
    "                                  keep_columns=['#SampleID', 'body_product'],\n",
    "                                  filters={'body_product': 'UBERON:feces'}\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ercolini whole grain feces\n",
    "\n",
    "This study [[4](#4)] contains fecal samples from 80 healthy overweight/obese subjects, part of which undergoing whole grain wheat dietery intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "erc_meta, erc_sotu = load_studies(biom_fp='../data/erc.qiita-1481.biom',\n",
    "                                  meta_fp='../data/erc.qiita-1481.txt',\n",
    "                                  name='ERC',\n",
    "                                  preservation='FRESH_FROZEN',\n",
    "                                  keep_columns=['env_matter', '#SampleID'],\n",
    "                                  filters={'env_matter': 'ENVO:feces'}\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate growth for bacteria in storage studies\n",
    "For each sOTU, calculate the mean fold change over all individuals, using a minimal read cutoff of 5 reads/sample to reduce effect of discretization and multinomial sampling of low frequency sOTUs.\n",
    "\n",
    "We'll start by defining the function we'll use to calculate these fold differences. We'll take a map and sOTU table for a storage study. We require that multiple samples at multiple time points exist per individual (designated by the `subject` column). We can compare the difference between timepoints (identied as the values of `time0` and `time1` and found in the `time_col`). Additional filters to select the sames, such as the perservative or storage temperature can be specified as well, using `filters`. The minimum sequencing depth for a sample to be considered is set as `seq_depth`, while the minimum number of counts for an OTU to be compared is `min_count`.\n",
    "\n",
    "The function will calculate the mean fold change in sOTUs of interest between the two time points for all OTUs which have at least hte minimum number of counts either before or after storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_stability(map_, otu_, time_col='time_numeric', time0='1', time1='2', \n",
    "                   subject_col='subject', mincount=5, seq_depth=2500, filters=None):\n",
    "    \"\"\"Calculates the fold change in a sub OTU between timepoints\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    map_ : DataFrame\n",
    "        The mapping file for the study. Must contain the columns\n",
    "        specified by `time_col`, `subject_col`, and any columns\n",
    "        used in `filters`.\n",
    "    otu_ : biom table\n",
    "        The sub OTU table assoicated with the study\n",
    "    time_col : str\n",
    "        The time in `map_` specifying when the sample was collected.\n",
    "    time0, time1: str, int\n",
    "        The values for the baseline and time change in the `time_col`\n",
    "    subject_col : str\n",
    "        A column in `map_` describing the way individuals are grouped\n",
    "        so that we can compare samples from the same person\n",
    "    min_count : int, optional\n",
    "        A mininum number of counts for caculating the ratio\n",
    "    seq_depth : int, optional\n",
    "        The minimum sequence depth for a sample to be analyzed\n",
    "    filters : None, dict\n",
    "        Any additional filters which should be applied to the data.\n",
    "        This should be expressed as a dictionary of column: value.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Series object keying the observation name to the mean fold\n",
    "        change between the two timepoints\n",
    "\n",
    "    \"\"\"\n",
    "    # Filters out samples without at least the minimum number of sequences\n",
    "    otu_.filter(lambda val, id_, md: val.sum() > seq_depth, axis='sample')\n",
    "    map_ = map_.loc[otu_.ids('sample')]\n",
    "        \n",
    "    # Sets up a filter for the mapping file to get the appropriate data.\n",
    "    def time_filter(x):\n",
    "        return (x[time_col] in {time0, time1})\n",
    "    if filters is not None:\n",
    "        def contraint_filter(x):\n",
    "            return np.all([x[c] == v for c, v in filters.items()])\n",
    "    else:\n",
    "        def contraint_filter(x):\n",
    "            return True\n",
    "    \n",
    "    # Filters the mapping file and otu table\n",
    "    map_ = map_.loc[map_.apply(time_filter, axis=1)].copy()\n",
    "    map_ = map_.loc[map_.apply(contraint_filter, axis=1).copy()]\n",
    "    otu_ = otu_.filter(map_.index, inplace=False)\n",
    "    otu_ = otu_.filter(lambda val, id_, md: (val.sum() > 0), axis='observation')\n",
    "    \n",
    "    otu_ids = otu_.ids('observation')\n",
    "    ratios = []\n",
    "    subjects = []\n",
    "    \n",
    "    # Groups by subject and iterates through the comparison\n",
    "    for host_subject_id, sub_map in map_.groupby(subject_col):\n",
    "        # Gets the list of samples at each time point\n",
    "        id0 = set(sub_map.index[sub_map[time_col] == time0])\n",
    "        id1 = set(sub_map.index[sub_map[time_col] == time1])\n",
    "        \n",
    "        # If there aren't samples for each timepoint, continue\n",
    "        if ((len(id0) == 0) or (len(id1) == 0)):\n",
    "            continue\n",
    "        \n",
    "        # Gets the sOTU data\n",
    "        sotus0 = otu_.filter(id0, inplace=False).sum(axis='observation') / len(id0)\n",
    "        sotus1 = otu_.filter(id1, inplace=False).sum(axis='observation') / len(id1)\n",
    "\n",
    "        low = (sotus0 < mincount) & (sotus1 < mincount)\n",
    "\n",
    "        sotus0[sotus0 < mincount] = mincount\n",
    "        sotus1[sotus1 < mincount] = mincount\n",
    "\n",
    "        ratio = sotus1 / sotus0\n",
    "        ratio[low] = np.nan\n",
    "        ratios.append(ratio)\n",
    "        subjects.append(host_subject_id)\n",
    "    \n",
    "    # Gets the list of ratios exlcuding any sOTU which doesnt meet the filtering criteria\n",
    "    ratios = pd.DataFrame(np.vstack(ratios), index=subjects, columns=otu_ids)\n",
    "    ratios.dropna(1, inplace=True, how='all')\n",
    "\n",
    "    # Calculate the mean fold change and the log fold change\n",
    "    fc = pd.Series(np.nanmean(ratios, 0), index=ratios.columns)\n",
    "    \n",
    "    return fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also generate a pretty histogram plot showing the comparison. This will take the fold change values and return a maplotlib axis with a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_logfoldchange_histogram(foldchange, fig=None, ax=None):\n",
    "    \"\"\"Makes a pretty histogram of the fold change\"\"\"\n",
    "    if fig is None:\n",
    "        fig = plt.figure()\n",
    "    if ax is None:\n",
    "        ax = plt.axes()\n",
    "        \n",
    "    # Constant limits and bins facilitate comparisons\n",
    "    sn.distplot(np.log2(foldchange.dropna()), ax=ax, bins=np.linspace(-12, 12, 120))\n",
    "    ax.set_xlim([-12, 12])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Sets up axis labels\n",
    "    ax.set_ylabel('Number of OTUs', size=24)\n",
    "    ax.set_xlabel('log$_{2}$(fold change)', size=24)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the fold change in the mayo stability study. We'll compare the samples from day 0 to day 1 and day 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mayo_fc  = pd.DataFrame(data=[test_stability(mayo_meta, mayo_sotu, \n",
    "                                             time_col='SAMPLE_VISIT', \n",
    "                                             subject_col='HOST_SUBJECT_ID',\n",
    "                                             time0='Day 0', \n",
    "                                             time1='Day 1'),\n",
    "                              test_stability(mayo_meta, mayo_sotu, \n",
    "                                             time_col='SAMPLE_VISIT', \n",
    "                                             subject_col='HOST_SUBJECT_ID',\n",
    "                                             time0='Day 0', \n",
    "                                             time1='Day 4')],\n",
    "                        index=['Day 1', 'Day 4']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the change between day 0 and day 1 in the same study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # mayo1_fig = plot_logfoldchange_histogram(mayo_fc['Day 1'])\n",
    "# mayo1_fig.savefig('../results/fig1-hist-mayo-1-0.pdf')\n",
    "\n",
    "# # Let's look at the difference between day 0 and day 4 in samples from the Mayo Stability study.\n",
    "\n",
    "# # mayo 4 days\n",
    "# mayo4_fig = plot_logfoldchange_histogram(mayo_fc['Day 4'])\n",
    "\n",
    "# mayo4_fig.savefig('../results/fig1-hist-mayo-4-0.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also look at the Song stability study, comparing fresh samples to those stored for one, two, and seven weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_fc  = pd.DataFrame(data=[test_stability(song_meta, song_sotu, \n",
    "                                             time0='1', \n",
    "                                             time1='2'),\n",
    "                              test_stability(song_meta, song_sotu, \n",
    "                                             time0='1', \n",
    "                                             time1='3'),\n",
    "                              test_stability(song_meta, song_sotu, \n",
    "                                             time0='1', \n",
    "                                             time1='4')],\n",
    "                              \n",
    "                         \n",
    "                        index=['Week 1', 'Week 2', 'Week 7']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sj7fc_fig = plot_logfoldchange_histogram(song_fc['Week 1'])\n",
    "# sj7fc_fig.savefig('../results/fig1-hist-sejin-7-0.pdf')\n",
    "\n",
    "# sj14fc_fig = plot_logfoldchange_histogram(song_fc['Week 2'])\n",
    "# sj14fc_fig.savefig('../results/fig1-hist-sejin-14-0.pdf')\n",
    "\n",
    "# # se jin 4 weeks\n",
    "# sj28fc_fig = plot_logfoldchange_histogram(song_fc['Week 7'])\n",
    "# sj28fc_fig.savefig('../results/fig1-hist-sejin-28-0.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Expression\n",
    "We'll next look at how American Gut, the shipped study, looks in comparison to the fresh frozen studies.\n",
    "\n",
    "So, we'll join all the cross sectional studies together and find sOTUs which differ significantly.\n",
    "Join all studies together and find sOTUs sginificantly higher/lower in AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combines the metadata\n",
    "fec_meta = pd.concat([pgp_meta[['STUDY', 'PRESERVATION']], \n",
    "                      twin_meta[['STUDY', 'PRESERVATION']], \n",
    "                      agp_meta[['STUDY', 'PRESERVATION']], \n",
    "                      erc_meta[['STUDY', 'PRESERVATION']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combines the sOTU tables\n",
    "fec_sotu_b = pgp_sotu.merge(erc_sotu).merge(twin_sotu).merge(agp_sotu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a function to allow us ... hash text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hash_sequence_ids(sequences):\n",
    "    \"\"\"Converts the sequence to a md5 hash\"\"\"\n",
    "    sequences['md5'] = sequences['sequence'].apply(\n",
    "        lambda x: hashlib.md5(x.encode('utf-8')).hexdigest()\n",
    "    )\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "def map_frozen_taxonomy(sequence):\n",
    "    tax = fec_sotu_b.metadata(sequence, axis='observation')['taxonomy']\n",
    "    return ';'.join(tax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the OTU table ot a pandas dataframe and do funny things to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fec_otu = exploding_panda(fec_sotu_b).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_otu.reset_index(inplace=True)\n",
    "sequences = fec_otu[['index']].copy() \n",
    "sequences.rename(columns={'index': 'sequence'}, inplace=True)\n",
    "freeze_thaw = hash_sequence_ids(sequences)\n",
    "freeze_thaw['taxonomy'] = freeze_thaw['sequence'].apply(map_frozen_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "md_key = freeze_thaw.set_index('sequence').to_dict()\n",
    "freeze_thaw.set_index('md5', inplace=True)\n",
    "\n",
    "fec_otu['index'] = fec_otu['index'].apply(lambda x: md_key['md5'][x])\n",
    "fec_otu = fec_otu.set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll perform log normalization percentile normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_sotu = fec_otu.apply(lambda x: 10000 * (x / x.sum()), axis=0)\n",
    "log_fec_sotu = fec_sotu.copy()\n",
    "log_fec_sotu[log_fec_sotu < 2] = 2\n",
    "log_fec_sotu = log_fec_sotu.loc[fec_sotu.sum(axis=1) > 0, :]\n",
    "log_fec_sotu = np.log2(log_fec_sotu)\n",
    "log_fec_sotu = log_fec_sotu.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a log transform to the data, and we'll calculate an effect size as the difference between the means of the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = permutation_mean(log_fec_sotu, fec_meta['PRESERVATION'], permutations=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `permutation_mean` method will return a dataframe. The column, `m` is the ***$\\left | \\bar_{x}_{1} - \\bar{x}_{2} \\right |$***. We'll calculate our effect size as this difference over the overall mean and then FDR-correct the permtuative p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res['mean'] = log_fec_sotu.mean(axis=0)\n",
    "res['effect_size'] = res.m / res['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res['corrected pvalue'] = multipletests(res.pvalue, method='fdr_bh')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our threshhold for rejection is an uncorrected p-value of 0.001 and an effect size of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res['reject'] = np.logical_and(res['pvalue'] <= 0.001,  res['effect_size'] > 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the fold change of american gut compared to all other experiments\n",
    "For each sOTU, comapre the mean in all AG samples to the mean of all samples in a fresh-frozen experiment. The samples are unpaired, so we can't just compare the means. We'll perform this calculation seperately between each pair of studies.\n",
    "\n",
    "We'll start by getting the pandas dataframes for each study to make the comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_sotu = fec_sotu.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pgp_meta_f = fec_meta.loc[fec_meta['STUDY'] == 'PGP']\n",
    "pgp_sotu_f = fec_sotu.loc[pgp_meta_f.index]\n",
    "pgp_sotu_f = pgp_sotu_f.reindex(index=pgp_meta_f.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agp_meta_f = fec_meta.loc[fec_meta['STUDY'] == 'AGP']\n",
    "agp_sotu_f = fec_sotu.loc[agp_meta_f.index]\n",
    "agp_sotu_f = agp_sotu_f.reindex(index=agp_meta_f.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twin_meta_f = fec_meta.loc[fec_meta['STUDY'] == 'UK_TWINS']\n",
    "twin_sotu_f = fec_sotu.loc[twin_meta_f.index]\n",
    "twin_sotu_f = twin_sotu_f.reindex(index=twin_meta_f.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "erc_meta_f = fec_meta.loc[fec_meta['STUDY'] == 'ERC']\n",
    "erc_sotu_f = fec_sotu.loc[erc_meta_f.index]\n",
    "erc_sotu_f = erc_sotu_f.reindex(index=erc_meta_f.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write a function to calculate the fold change between studies.\n",
    "\n",
    "**Check the mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_fold_change(sotu1, sotu2, minreads=1):\n",
    "    \"\"\"Calculates the fold change between two datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sotu1 : pd.DataFrame\n",
    "        A DataFrame representation of an sOTU table\n",
    "    sotu1 : pd.DataFrame\n",
    "        A DataFrame representation of an sOTU table\n",
    "    minreads : int\n",
    "        The minimum number of reads required in the sample \n",
    "        to calculate hte fold change\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replaces values below the threshhold\n",
    "    sotu1[sotu1 < minreads] = minreads\n",
    "    sotu2[sotu2 < minreads] = minreads\n",
    "    \n",
    "    # Calculates the mean ratio\n",
    "    ratio = sotu1.mean(0) / sotu2.mean(0)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's calculate the fold change ratio for all combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fold_change = pd.DataFrame([calculate_fold_change(agp_sotu_f, pgp_sotu_f),\n",
    "                            calculate_fold_change(agp_sotu_f, erc_sotu_f),\n",
    "                            calculate_fold_change(agp_sotu_f, twin_sotu_f)],\n",
    "                           index=['agp_pgp', 'agp_erc', 'agp_twins']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_ag_fc = fold_change.min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify candidate blooming bacteria list\n",
    "Iterate over all sOTUs which are significantly different between AG and all fresh-frozen samples. For each such sOTU, we calculate two values:\n",
    "\n",
    "1. The maximal growth potential (maximum over all storage studies and days)\n",
    "2. The minimal ratio between AG to each fresh-frozen sample. The minimum is taken in order to reduce the effect of study-study differences (since a blooming bacteria is expected to be higher in AG compared to all fresh-frozen studies)\n",
    "\n",
    "Candidate blooming bacteria are defined as sOTUs that satisfy either of the following criteria::\n",
    "\n",
    "* At least two fold difference in both steps (1) and (2)\n",
    "* At least two fold difference in step (2) and were not observed in step (1). \n",
    "* At least 50 fold change in step (1).\n",
    "\n",
    "The second criteria is present as the storage studies have a small set of individuals, and there may be blooming bacteria that by chance were not present in the storage study samples. The third criteria is so that we know they can grow drastically in room temp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the stability studies\n",
    "\n",
    "We're going to find the maximum fold difference for all comparisons based on storage studies.\n",
    "\n",
    "Unfortunately, the Song et al study reads are 125 nucleotides long instead of the 150 nucelotides found in the other studies. However, these samples can still be used conservatively by comparing the first 125 nucleotides of each of the significant sequences we found in the freeze-thaw studies with the sequences in the song study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_seqs = pd.DataFrame({'sequence': sequences.loc[res.loc[res.reject].index].sequence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_seqs['sequence125'] = diff_seqs.sequence.apply(lambda x: x[:124])\n",
    "diff_seqs.reset_index(inplace=True)\n",
    "diff_seqs.rename(columns={'index': 'md5'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_fc.reset_index(inplace=True)\n",
    "song_fc.rename(columns={'index': 'sequence125'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_diff_fc = pd.merge(song_fc, diff_seqs, on='sequence125', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also add in the Mayo stability study, with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mayo_fc.reset_index(inplace=True)\n",
    "mayo_fc.rename(columns={'index': 'sequence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mayo_diff_fc = pd.merge(mayo_fc, diff_seqs, on='sequence', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storage_fc = pd.merge(song_diff_fc, mayo_diff_fc, on=['md5', 'sequence'], \n",
    "                      how='outer', suffixes = ('_song', '_mayo')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can combine the studies, and then combine the sequences. We'll use the Mayo sequences over the storage sequences, since these are longer and therfore easier to specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the fold change between the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storage_fc.index = storage_fc.md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "storage_max = storage_fc[['Week 1', 'Week 2', 'Week 7', 'Day 1', 'Day 4']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extremes = pd.DataFrame({\"storage\":storage_max, \n",
    "                         \"freeze_thaw\":min_ag_fc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_seqs.index = diff_seqs['md5']\n",
    "extremes = extremes.loc[res.index[res['reject']]]\n",
    "extremes = pd.merge(extremes, diff_seqs, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def identify_blooms(x):\n",
    "    return (((x['storage'] > 2) & (x['freeze_thaw'] > 2)) | \n",
    "            (x['storage'] > 50) |\n",
    "            ((x['freeze_thaw'] > 2) & (np.isnan(x['storage'])))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blooms = extremes.index[extremes.apply(identify_blooms, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the fold change scatter plot KDE with highlighting the candidate blooming sOTUs\n",
    "since we have a lot of sOTUs, each with max(storage fc) and min(ag-fresh fc), we plot the average densities of all sOTUs.\n",
    "\n",
    "Red circles denote the sOTUs (15) passing the criteria for candidate blooming bacteria. Note that an additional set of 6 candidate blooming bacteria are not shown as they do not appear in the storage studies, and therefore do not have an x coordinate on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extremes = pd.merge(extremes, res, \n",
    "                    left_index=True, right_index=True)\n",
    "clean_extremes = np.log2(extremes[['storage', 'freeze_thaw']]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "fig = ax.figure\n",
    "sn.kdeplot(clean_extremes['storage'],\n",
    "           clean_extremes['freeze_thaw'],\n",
    "           shade=True,\n",
    "           cmap='Greys',\n",
    "           ax=ax)\n",
    "ax.scatter(clean_extremes.loc[blooms, 'storage'],\n",
    "           clean_extremes.loc[blooms, 'freeze_thaw'],\n",
    "           s=80, color=sn.color_palette()[2])\n",
    "ax.set_xlim([-3, 9])\n",
    "ax.set_ylim([-5, 6.25])\n",
    "ax.set_xlabel('max. storage study foldchange (log2)',fontsize=16)\n",
    "ax.set_ylabel('min. AG foldchange (log2)',fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.savefig('../results/fig1-blooming-identification-kde.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lambda x: x['freeze_thaw']*0.01 + x['storage']\n",
    "extremes = extremes.fillna(0.0)\n",
    "extremes['score'] = extremes.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extremes = pd.merge(extremes, res, \n",
    "                    left_index=True, right_index=True)\n",
    "order_list = extremes.loc[blooms].sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bloom_seqs = []\n",
    "bloom_taxa = []\n",
    "\n",
    "for idx, extreme_seq in enumerate(order_list.index):\n",
    "    bloom_seqs.append(\n",
    "        '>%s\\n%s' % (extreme_seq, freeze_thaw.loc[extreme_seq, 'sequence'])\n",
    "        )\n",
    "    bloom_taxa.append(\n",
    "        '%s\\t%s' % (extreme_seq, freeze_thaw.loc[extreme_seq, 'taxonomy'])\n",
    "        )\n",
    "\n",
    "    print('%0d %f %s' % (idx + 1, \n",
    "                         extremes.loc[extreme_seq, 'score'],\n",
    "                         freeze_thaw.loc[extreme_seq, 'taxonomy'],\n",
    "                         ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we're going to write the sequences, and a subset of the sequences to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/newbloom.all.fna', 'w') as f_:\n",
    "    f_.write('\\n\\n'.join(bloom_seqs))\n",
    "\n",
    "with open('../data/newbloom.8.fa', 'w') as f_:\n",
    "    f_.write('\\n\\n'.join(bloom_seqs[:8]))\n",
    "    \n",
    "with open('../results/bloom_taxa.txt', 'w') as f_:\n",
    "    f_.write('\\n\\n'.join(bloom_taxa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxa Plots\n",
    "\n",
    "Finally, we're going to look at the change in taxonomy across the four cross sectional experiments before and after bloom filtering. We'll first filter out the bloom sequences to identify the post-bloom table. To do this, we'll first establish a list of high abundance classes, which we'll use in the collapse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_keep = ['c__Gammaproteobacteria',\n",
    "            'c__Clostridia',\n",
    "            'c__Bacilli',\n",
    "            'c__Erysipelotrichi',\n",
    "            'c__Bacteroidia',\n",
    "            'c__Actinobacteria',\n",
    "            'c__Mollicutes',\n",
    "            'c__Other'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out the bloom sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extreme_seqs = set(freeze_thaw.loc[order_list.index, 'sequence'].values)\n",
    "fec_sotu_filter = fec_sotu_b.filter(extreme_seqs, invert=True, axis='observation', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to partition and sum the classes, to look at the taxonomic distribution before and after filtering, and use these to generate data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_biom(id_, md):\n",
    "    tax = md['taxonomy']\n",
    "    if len(tax) > 2 and tax[2] in sub_keep:\n",
    "        return tax[2]\n",
    "    else:\n",
    "        return 'c__Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_ = pd.DataFrame(\n",
    "    {id_: tab.sum(axis='sample') for id_, tab in \n",
    "     fec_sotu_b.norm().partition(partition_biom, axis='observation')},\n",
    "    index=fec_sotu_b.ids('sample')\n",
    "    )\n",
    "post = pd.DataFrame(\n",
    "    {id_: tab.sum(axis='sample') for id_, tab in \n",
    "     fec_sotu_filter.norm().partition(partition_biom, axis='observation')},\n",
    "    index=fec_sotu_filter.ids('sample')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll add study information and group by the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_['STUDY'] = fec_meta['STUDY']\n",
    "post['STUDY'] = fec_meta['STUDY']\n",
    "\n",
    "pre_class = pre_.groupby('STUDY').mean()[sub_keep]\n",
    "postclass = post.groupby('STUDY').mean()[sub_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots the taxonomy plots\n",
    "\n",
    "We'll write a quick function to help make the stacked bar chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_taxa_boxplot(taxa, order, ax, colors=None):\n",
    "    \"\"\"Plots a barchart of the taxonomy   \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    taxa: DataFrame\n",
    "        A pandas dataframe with the samples or studies in\n",
    "        the index and the clades in the columns\n",
    "    order : array-like\n",
    "        The order in which columns should be plotted\n",
    "    ax : Axes\n",
    "        A matplotlib axis object where the data should be plotted\n",
    "    colors : None, array-like, optional\n",
    "        A valid matplotlib color representation at least as long\n",
    "        as the taxa order.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        A stacked taxonomy barchart will be plotted on the axis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gets the color palete\n",
    "    if colors is None:\n",
    "        colors = sn.color_palette('Set1', len(order))\n",
    "\n",
    "    taxa = taxa[order]\n",
    "    low = np.cumsum(taxa, axis=1) - taxa\n",
    "    \n",
    "    # Plots the boxplot\n",
    "    for class_, color in zip(*(sub_keep, colors)):\n",
    "        ax.bar(left=np.arange(0, len(taxa.index)) + 0.1,\n",
    "               bottom=low[class_],\n",
    "               height=taxa[class_],\n",
    "               width=0.8,\n",
    "               color=color,\n",
    "               )\n",
    "\n",
    "    ax.set_xticks(np.arange(0, len(taxa.index)) + 0.5)\n",
    "    ax.set_xticklabels([t.replace('_', ' ') for t in taxa.index])\n",
    "    \n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('Relative Abundance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a colormap which will highlight the gamma proteobacteria in the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = sn.color_palette('Pastel1', 7)\n",
    "colors[0] = sn.color_palette('Set1', 2)[0]\n",
    "colors.append([0.85, 0.85, 0.85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the pre and post filtering figures and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig_tb, ax = plt.subplots(1, 1)\n",
    "make_taxa_boxplot(pre_class, sub_keep, ax, colors)\n",
    "fig.savefig('taxa_pre_filtering.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig_ta, ax = plt.subplots(1, 1)\n",
    "make_taxa_boxplot(postclass, sub_keep, ax, colors)\n",
    "fig.savefig('../results/taxa_post_filtering.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll generate a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig_lt, ax = plt.subplots(1, 1)\n",
    "ax.set_position((0.4, 0.4, 0.2, 0.2))\n",
    "pie_ = ax.pie([1 / len(sub_keep)] * len(sub_keep), colors=colors)\n",
    "l = ax.legend(labels=[c.replace('c__', '') for c in sub_keep],\n",
    "              frameon=True,\n",
    "              fontsize=15)\n",
    "plt.savefig('../results/taxa_legend.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In conclusion, we can take advantage of the microbes identified to be blooming in previous studies, combined with comparison between shipped and fresh frozen samples, to identify candidate taxa that bloom in storage conditions in the American Gut project.\n",
    "\n",
    "An advantage of performing such analysis on a sub-OTU level (i.e. the output of the Deblur algorithm), is that we can identify and remove the exact blooming bacterial sequences, whereas using traditional OTU based methods would require removal of the blooming bacteria and all close sequences, which can lead to much higher numbers of bacteria removed.\n",
    "\n",
    "Note that choosing the number of candidate blooming bacteria to remove is a type-1/type-2 balance. Removal of more bacteria will reduce the effect of shipment, at the cost of losing more bacteria for analysis.\n",
    "\n",
    "While all bacteria may change during shipment, it seems only a small number of them exhibit a drastic growth. By removal of as little as the top 8 blooming bacteria, we can greatly reduce the effect of shipment. This leads to a great reduction in the noise within American Gut samples (since not all samples are undergo the same shipment conditions/duration), as well as enable comparison to other, fresh frozen studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "<ol><li><a id='1'></a>Song, S.J., Amir, A.; Metcalf, J.L.; Amato, K.R.; Xu, Z.Z.; Humphrey, G.; Knight, R. (2016). \"<a href=\"http://msystems.asm.org/content/1/3/e00021-16\">Preservation Methods Differ in Fecal Microbiome Stability, Affecting Suitability for Field Studies</a>. <em>mSystems</em>. 1: e00021-16\n",
    "</li><li><a id=\"2\"></a>Sinha, R.; Chen, J.; Amir, A,; Vogtmann, E.; Shi, J.; Inman, K.S.; Flores, R.; Sampson, J.; Knight, R.; Chia, N. (2016) \"<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26604270\">Collecting fecal samples for microbiome analyses in epidemiology studies.</a>\" <em>Cancer Epidemiol Biomarkers Prev</em> 25: 407-416.\n",
    "</li><li><a id=\"3\"></a>Goodrich, J.K.; Waters, J.L.; Poole, A.C.; Sutter, J.L.; Blekhman, R.; Beaumont, M.; Van Treuren, W.; Knight, R.; Bell, J.T.; Spector, T.D.; Clark, A.G.; Ley, R.E. (2014). \"<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/25417156\">Human genetics shape the gut microbiome</a>.\" <em>Cell</em>. 159: 789-799.\n",
    "</li><li><a id=\"4\"></a>Vitaglione, P.; Mennella, I.; Ferracane, R.; Rivellese, A.A.; Giacco, R.; Ercolini, D.; Gibbons, S.; La Storia, A.; Gilbert, J.A.; Jonnalagadda, S.; Thielecke, F.; Galo, M.A.; Scalfi, L.; Fogliano, V. (2015). \"<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/25646321\">Whole-grain wheat consumption reduces inflammation in a randomized controlled trial on overweight and obese subjects with unhealthy dietary and lifestyle behaviors: role of polyphenols bound to cereal dietary fiber</a>.\" <em>Am J Clin Nutr</em> 101: 251-261.\n",
    "</li><li><a id=\"5\"></a>Mandal, S.; Van Treuren, W.; White, R.A.; Eggesbo, M.; Knight, R.; Peddada, S.D (2015) \"<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26028277\">Analysis of composition of microbiomes: a novel method for studying microbial composition</a>.\" <em>Microb Ecol Health Dis</em> 29:27663.\n",
    "</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
